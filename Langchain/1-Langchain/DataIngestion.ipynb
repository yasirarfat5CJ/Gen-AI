{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa02302a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x7f30442fa050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text loadder \n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader('speech.txt')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5e1581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Speech: Believe in Yourself and Keep Moving Forward\\nGood morning respected teachers and my dear friends,\\nToday, I want to talk about something very simple yet very powerful — belief in oneself.\\nMany of us dream big. We want success, respect, a good career, and a meaningful life. But often, the biggest obstacle is not the world outside us — it is the doubt inside us. We ask ourselves: Am I good enough? Can I really do this?\\nLet me tell you something important:\\nEvery successful person you admire once stood exactly where you are standing today — confused, unsure, and afraid of failure.\\nFailure is not the opposite of success. Failure is a part of success.\\nEach mistake teaches us something. Each setback makes us stronger. What matters is not how many times we fall, but how many times we rise again.\\nIn student life, pressure is everywhere — exams, placements, expectations, and comparisons. But remember, everyone has their own journey and their own pace. Do not compare your beginning with someone else’s middle.\\nHard work, consistency, and patience always pay off. Even if results take time, the effort you put in today is shaping your future.\\nSo, believe in yourself when no one else does.\\nKeep learning, keep improving, and never give up on your dreams.\\nI would like to end with this line:\\n“Small steps every day lead to big success one day.”\\nThank you, and have a wonderful day.\\n\\n\\n')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_doc=loader.load() ## used for creating an text document\n",
    "text_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca5abbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 0, 'page_label': '1'}, page_content='1 \\n \\nUNIT-I \\nDigital Computers: Introduction, Block diagram of Digital Computer, Definition of Computer \\nOrganization, Computer Design and Computer Architecture. \\nRegister Transfer Language and Micro operations: Register Transfer language, Register Transfer, \\nBus and memory transfers, Arithmetic Micro operations, logic micro operations, shift micro operations, \\nArithmetic logic shift unit. \\nBasic Computer Organization and Design: Instruction codes, Computer Registers Computer \\ninstructions, Timing and Control, Instruction cycle, Memory Reference Instructions, Input – Output and \\nInterrupt. \\n------------------------------------------------------------------------------------ \\nThe digital computer: \\n➢ The digital computer is a digital system that performs various computational tasks. \\n➢ The word digital implies that the information in the computer is represented by variables that take a \\nlimited number of discrete values.  \\n➢ The first electronic digital computers, developed in the late 1940s, were used primarily for \\nnumerical computations.  \\n➢ In this case the discrete elements are the digits. From this application the term digital computer has \\nemerged. \\n➢  In practice, digital computers function more reliably if only two states are used. Digital computers \\nuse the binary number system, which has two digits: 0 and 1. A binary digit is called a bit. \\n➢ Information is represented in digital computers in groups of bits. By using various coding \\ntechniques, groups of bits can be made to represent not only binary numbers but also other d iscrete \\nsymbols. \\n➢ In contrast to the common decimal numbers that employ the base 10 system, binary numbers use a \\nbase 2 system with two digits: 0 and I. The decimal equivalent of a binary number can be found by \\nexpanding it into a power series with a base of 2.  \\n➢ For example, the binary number 1001011 represents a quantity that can be converted to a decimal \\nnumber by multiplying each bit by the base 2 raised to an integer power as follows: \\n \\n➢ The seven bits 1001011 represent a binary number whose decimal equiv alent is 75. However, this \\nsame group of seven bits represents the letter K when used in conjunction with a binary code for the \\nletters of the alphabet.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 1, 'page_label': '2'}, page_content='2 \\n \\n \\n❖ A computer system is sometimes subdivided into two functional entities:  \\n     hardware and software.  \\nThe hardware of the computer is usually divided into three major parts, as shown in Fig. 1 -A. The \\ncentral processing unit (CPU) contains an arithmetic and  logic unit for manipulating data, a number of \\nregisters for storing data, and control circuits for fetching and executing instructions. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1-A: Block diagram of a digital computer. \\n \\n➢ The memory of a computer contains storage for instructions and data. It is called a random access \\nmemory (RAM) because the CPU can access any location in memory at ra ndom and retrieve the \\nbinary information within a fixed interval of time.  \\n➢ The input and output processor (lOP) contains electronic circuits for communicating and controlling \\nthe transfer of information between the computer and the outside world. \\n➢ The input  and output devices connected to the computer include keyboards, printers, terminals, \\nmagnetic disk drives, and other communication devices. \\n \\n❖ Computer organization is concerned with the way the hardware components operate and the way \\nthey are connected together to form the computer system. The various components are assumed to be \\nCentral Processing Unit \\nCPU \\nInput-output Processor \\n(IOP) \\nRandom-access-memory \\nRAM \\nInput \\nDevices \\nOutput \\nDevices'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 2, 'page_label': '3'}, page_content='3 \\n \\nin place and the task is to investigate the organizational structure to verify that the computer parts \\noperate as intended. \\n \\n❖ Compute design  is concerned with the hardware design of the computer. Once the computer \\nspecifications are formulated, it is the task of the designer to develop hardware for the system. \\nComputer design is concerned wit h the determination of what hardware should be used and how the \\nparts should be connected. This aspect of computer hardware is sometimes referred to as computer \\nimplementation. \\n❖ Computer architecture is concerned with the structure and behavior of the computer as seen by the \\nuser. It includes the information formats, the instruction set, and techniques for addressing memory. \\nThe architectural design of a computer system is concerned with the specifica tions of the various \\nfunctional modules, such as processors and memories, and structuring them together into a computer \\nsystem. \\nRegister Transfer language: \\n❖ Register: A register may hold an instruction, a storage address, or any kind of data (such as a bit \\nsequence or individual characters). \\n❖ Computer registers are designated by capital letters (sometimes followed by  numerals) to denote the \\nfunction of the register.  \\n❖ For example, the register that  holds an address for the memory unit is usually called a memor y \\naddress register and is designated by the name MAR. \\n❖ The most common way to represent a register is by a rectangular box with the name of the register \\ninside, as in Fig. I -(a). The individual bits can be distinguished as in (b). The numbering of bits in a \\n16-bit register can be marked on top of the box as shown in (c). A  16-bit register is partitioned into \\ntwo parts in (d).  \\n \\n \\n \\n \\n \\nFig:  I-Block diagram of registers'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 3, 'page_label': '4'}, page_content='4 \\n \\n❖ Bits 0 through 7 are assigned the symbol L (for low byte) and bits 8 through 15 are assigned the \\nsymbol H (for high byte). The name of the 16 -bit register is PC . The symbol PC(0 -7) or PC(L) \\nrefers to the low-order byte and PC(S-15) or PC( H) to the high-order byte. \\n❖ Micropoeration: The operations executed on data stored in registers are called micro operations. \\n✓ A micro operation is an elementary operation performed on the info rmation stored in one  or \\nmore registers.  \\n✓ The result of the operation may replace the previous binary  information of a register or may \\nbe transferred to another register.  \\n✓ Examples of micro operations are shift, count, clear, and load. \\n❖ Register transfer language - The symbolic notation used to descri be the micro operation transfers \\namong registers is called a register transfer language.  \\n✓ The term \"register  transfer\" implies the availability of hardware logic circuits that can \\nperform a  stated micro operation  and transfer the result of the operation to the same or  \\nanother register. \\n✓ A register transfer language is a system for expressing in  symbolic form the micro operation \\nsequences among the registers of a digital module. \\n❖ Information transfer from one register to another is designated in symbolic form by means of a \\nreplacement operator. \\n The statement \\nR2 \\uf0dfR1 \\ndenotes a transfer of the content of register R1 into register R2. It designates a replacement of the \\ncontent of R2 by the content of R l . By definition, the  content of the source register R1 does not \\nchange after the transfer \\n❖ Normally, we want the transfer to occur only under a predetermined control condition. This can be \\nshown by means of an if-then statement. \\n            If (P = 1) then (R2 \\uf0df R1)          /* Register transfer with control function */ \\n                   Where P is a control signal generated in the control section \\n✓ A control function is a Boolean variable that is  equal to 1 or 0. The control function is included \\nin the statement as follows: \\n                      P: R2 \\uf0dfR1'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 4, 'page_label': '5'}, page_content='5 \\n \\nBus and memory transfers: \\n• A typical digital computer h a s many registers, and paths must b e provided to transfer \\ninformation from one register to another. \\n• The number of wires will be excessive if separate lines are used between each register and all \\nother registers in the system \\n• A more efficient scheme for transferring information between registers in a multiple -register \\nconfiguration is a common bus system. \\nCommon Bus System: \\n➢ One way of constructing a comm on bus system is with multiplexers. The multiplexers select the \\nsource register whose binary information is then placed on the bus.  \\n➢ The construction of a bus system for four registers is shown in Fig. I -B. Each register has four \\nbits, numbered 0 through 3. \\n➢  The bus consists of four 4 x 1 multiplexers each having four data inputs, 0 through 3, and two  \\nselection inputs, S1 and S0  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig I-B:  Bus system for four registers'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 5, 'page_label': '6'}, page_content='6 \\n \\nNOTE: In order not to complicate the diagram with 16 lines crossing each other, we use labels to \\nshow the connections from the outputs of the registers to the inputs of the multiplexers. For example, \\noutput 1 of register A is connected to input 0 of MUX 1 because this input is labeled A1. \\n \\n➢ The diagram shows that the bits in the same significant position in each register  are connected to \\nthe data inputs of one multiplexer to form one line of the bus.  Thus MUX 0 multiplexes the four 0 \\nbits of the registers, MUX 1 multiplexes the four 1 bits of the registers, and similarly for the other \\ntwo bits. \\n➢ The two selection lines S1 and S0 are connected to the selection inputs of all four multiplexers.  \\n✓ The selection lines choose the four bits of one register  and transfer the m into the four -line \\ncommon bus.  \\n✓ When S1S0 = 00, the 0 data inputs of all four multiplexers  are selected and applied to the \\noutputs that form the bus.  \\n✓ This causes the bus lines to receive the content of register A since the outputs of this register \\nare c onnected to the 0 data inputs of the multiplexers. Similarly, register B is selected if \\nS1S0 = 01, and so on. \\n➢ Table I-C shows the register that is selected by the bus for each of the four possible binary value \\nof the selection lines \\n \\n \\n \\n \\n \\n \\n              Table I-C: Function Table for Bus of Fig. I-B \\nNOTE: In general, a bus system will multiplex k registers of n bits each to produce an n -line common \\nbus. The number of multiplexers needed to construct the bus is equal to n , the number of bits i n each \\nregister. The size of each multiplexer must be k x 1 since it multiplexes k data lines. For example, a \\ncommon bus for eight registers of 16 bits each requires 16 multiplexers, one for each line in the bus. \\nEach multiplexer must have eight data input  lines and three selection lines to multiplex one significant \\nbit in the eight registers.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 6, 'page_label': '7'}, page_content='7 \\n \\nThree (Tri) - State Buffer: \\n➢ A bus system can be constructed with three-state gates instead of multiplexers.  \\n➢ A three-state gate is a digital circuit that exhibits three states. Two of the states are signals equivalent \\nto logic 1 and 0 as in a conventional gate.  \\n➢ The third state is a high-impedance state. The high -impedance stat e behaves like an open circuit, \\nwhich means that the output is disconnected and does not have logic significance.  \\n➢ The graphic symbol of a three -state buffer gate is shown in Fig. I -D. It is  distinguished from a \\nnormal buffer by having both a normal input and a control input.  \\n \\n \\n \\nFig. I-D : Graphic symbols for three- state buffer \\n➢ The control input determines the output state. When the control input  is equal to 1, the output is \\nenabled and the gate behaves like any conventional buffer, with the output equal to the normal input. \\nWhen the control input is  0, the output is disabled and the gate goes to a high -impedance state, \\nregardless of the value in the normal input. \\nNOTE: Q: What is the use of Tri-state buffer? Or what is the application of Tri-state Buffer? \\nAnswer: To designing the Common bus system  \\n \\nConstruction of a bus system with three-state buffers: \\nThe construction of a bus system with three-state buffers is demonstrated in Fig. I-E.  \\n➢ The outputs of four buffers are connected together to form a single  bus line. (It must be realized that \\nthis type of connection cannot be done with gates that do not have three-state outputs. )  \\n➢ The control inputs to the buffers  determine which of the four nor mal inputs will communicate with \\nthe bus line.  \\n➢ No more than one buffer may be in the active state at any given time.  \\n➢ The connected buffers must be controlled so that only one three -state buffer has access to the bus \\nline while all other buffers are maintained in a high -impedance state \\nTo construct a common bus for four registers of n bits  each using three- state buffers, we need n circuits \\nwith four buffers in each as shown in Fig. I-E.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 7, 'page_label': '8'}, page_content='8 \\n \\nEach group of four buffers receives one significant bit from the fou r registers.  Each common output \\nproduces one of the lines for the common bus for a total  of n lines. Only one decoder is necessary to \\nselect between the four registers \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n                                  Figure I-E:  Bus line with three state-buffers. (SINGLE BIT)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 8, 'page_label': '9'}, page_content='9 \\n \\n \\nNOTE: Here E in decoder is to enable the decoder if E=1 or if E=0 it is disable. \\n• Select lines are input lines (S0S1) are to select one of the four outputs \\n• Here if you observe We are considering four registers with 4 -bit each(i.e A0,A1,A2,A3 and \\nB0.B1,B2,B3 and C0,C1,C2,C3 and D0,D1,D2,D3). \\n• In above figure we are only representing 0 bit  (A0,B0,C0,D0) of all four registers similarly we can \\nrepresents All 1 bit (A1,B1,C1,D1) of four register, and so on. \\nHere the figure showing the complete four bits of four register with three state buffer  is'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 9, 'page_label': '10'}, page_content=\"10 \\n \\nArithmetic Micro operations: \\nIn general, the Arithmetic Micro-operations deals with the operations performed on numeric data stored \\nin the registers. \\n➢ Add Micro-Operation :It is defined by the following statement: \\nR3\\uf0df R1 + R2 \\nThe above statement instructs the contents of register R1 and content of register R2 should be \\ntransferred to register R3. \\n➢ Subtract Micro-Operation :Let us again take an example: \\nR3\\uf0df R1- R2 \\nR3 \\uf0df  R1 + R2' + 1 (2’s complement) \\nIn subtract micro-operation, instead of using minus operator we take 1's compliment and add 1 to the \\nregister which gets subtracted, i.e R1 - R2 is equivalent to R3 \\uf0df R1 + R2' + 1. \\nThe following table shows the symbolic representation of various Arithmetic Micro-operations. \\nSymbolic Representation                                  Description \\nR3 ← R1 + R2 The contents of R1 plus R2 are transferred to R3. \\nR3 ← R1 - R2 The contents of R1 minus R2 are transferred to R3. \\nR2 ← R2' Complement the contents of R2 (1's complement) \\nR2 ← R2' + 1 2's complement the contents of R2 (negate) \\nR3 ← R1 + R2' + 1 R1 plus the 2's complement of R2 (subtraction) \\nR1 ← R1 + 1 Increment the contents of R1 by one \\nR1 ← R1 - 1 Decrement the contents of R1 by one \\n \\nBinary adder: \\n➢ The digital circuit that generates the arithmetic sum of two binary numbers of any length is called a \\nbinary adder. \\n➢ The binary adder is constructed with full -adder circuits connected in cascade, with the output carry \\nfrom one full-adder connected to the input carry of the next full-adder.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 10, 'page_label': '11'}, page_content='11 \\n \\n➢ Figure I-F shows the interconnections of four full-adders (FA) to provide a 4-bit binary adder. \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure I-F: 4-bit binary adder. \\n \\n➢ The augend bits of A and the addend bits of B are designated by subscript numbers from right to left, \\nwith subscript 0 denoting the low-order bit. \\n➢  The carries are connected in a chain through the full -adders. The input carry to the binary adder is \\nC0 and the output carry is C4. The S outputs of the full-adders generate the required sum bits. \\nNumerical Example:  \\nLet us consider two 4-bit binary adder of two registers \\nR1  --- >    1 1 0 1 ------------- >augmend \\nR2 ------> 0 1 1 1-------------- > addend \\nWe need to perform addition of both R1+R2. i.e  \\n \\n \\n \\n                                                                    \\n                                                                 \\n \\n \\n \\n➢ An n-bit binary adder requires n full-adders.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 11, 'page_label': '12'}, page_content=\"12 \\n \\nBinary Adder-Subtractor: \\n➢ The addition and subtraction operations can be combined into one common circuit by including an \\nexclusive-OR gate with each full-adder.  \\nNOTE: Remember that the subtraction A – B can be done by taking the 2's complement of B and \\nadding it to A. The 2's com plement can be obtained by taking the 1' s complement and adding one \\nto the least significant pair of bits. The 1's complement can be implemented with inverters and a \\none can be added to the sum through the input carry. \\n \\n➢ A 4-bit adder-subtractor circuit is shown in Fig. I-G. \\n \\n  \\n \\n \\n \\n  \\n \\n \\nFigure I-G:4-bit adder-subtractor \\nThe mode input S controls the operation. When S = 0 the circuit is an adder and when S = 1 the circuit \\nbecomes a subtractor.  Each exclusive-OR gate receives input M and one of the inputs of B.  \\n➢ When S = 0, we have B \\uf0c5 0 = B. The full-adders receive the value of B, the input carry is 0 , and the \\ncircuit performs A plus B.  \\n➢ When S = 1, we have B \\uf0c5 1 = B' and C0 = 1. The B inpu ts are all complemented and a 1 is added \\nthrough the input carry .The circuit performs the operation A plus the 2's complement of B.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 12, 'page_label': '13'}, page_content='13 \\n \\nBinary lncrementer: \\n➢ The increment micro operation adds one to a number in a register.  \\n➢ For example, if a 4-bit register has a binary value 1101, it will go to 1110 after it is incremented. \\n \\n \\n \\n \\n \\n➢ The diagram of a 4-bit combinational circuit incrementer is shown in Fig. 1-H. \\n➢ One of the inputs to the least significant half -adder (HA) is connected to logic -1 and the other \\ninput is connected to the least significant bit of the number to be incremented \\n➢ The output carry from one half-adder is connected to one of the inputs of the next-higher-Order \\nhalf-adder.  \\n➢ The circuit receives the four bits from A 0, through A 3, adds one to it , and generates the \\nincremented output in S0 through S3. \\n \\n \\n \\n \\n \\n \\nFigure I-H: 4-bit binary incrementer. \\nNOTE: For n-bit binary incrementer by extending the diagram to include n half-adders \\nArithmetic Circuit: \\n• The arithmetic microoperations can be implemented in one composite arithmetic circuit. \\n• The basic component of an arithmetic circuit is the parallel adder. By controlling the data inputs to \\nthe adder, it is possible to obtain different types of arithmetic operations.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 13, 'page_label': '14'}, page_content='14 \\n \\n \\n❖ The diagram of a 4-bit arithmetic circuit is shown in Fig. I -I. It has four  full-adder circuits that \\nconstitute the 4-bit adder and four multiplexers for choosing different operations.  \\n❖ There are two 4-bit inputs A and B and a 4-bit output D. The four inputs from A go directly to the X \\ninputs of the binary adder.  \\n❖ Each of the four inputs from B are connected to the data inputs of the multiplexers( i.e 0 input bit).  \\n❖ The multiplexers data inputs also receive the complement of B(i.e 1 input bit) . \\n❖ The other two data inputs (i.e input bits 2 and  3  of the each multiplexer are co nnected to logic -0 \\nand logic-1. \\n❖ The four multiplexers are controlled by two selection inputs, S1 and S0.  \\n❖ The input carry C in goes to the  carry input of the FA in the least significant position. The other \\ncarries are connected from one stage to the next.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure I-I:  4-bit arithmetic circuit'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 14, 'page_label': '15'}, page_content=\"15 \\n \\n❖ The output of the binary adder is calculated from the following arithmetic sum: \\nD = A + Y + Cin \\n Where, \\n→ A is the 4-bit binary number at the X inputs  \\n→ Y is the 4-bit binary number at the Y inputs of the binary adder. \\n→ Cin is the input carry, which can be equal to 0 or 1.  \\n❖ Here some are possible micro arithmetic operations listed below Table I-J \\n \\n \\n \\n \\n \\n \\n❖ When S1S0 = 00, the value of B is applied to the Y inputs of the adder. \\n✓ If Cin = 0, the output D = A + B . If Cin = 1, output D = A + B + l.  \\n✓ Both cases perform the add micro operation with or without adding the input carry. \\n❖ When S1S0 = 01, the complement of B is applied to the Y inputs of the adder.  \\n✓ If Cin = 1, then D  = A +   B̅  + 1. This produces A pl us t he 2's complement of B, which is \\nequivalent to a subtraction of A - B.  \\n✓ If  Cin = 0, then D = A +  B̅ . This is equivalent to a subtract with borrow. \\n❖ When S1S0 = 10, the inputs from B are neglected, and instead, all 0's are inserted into the Y inputs. \\nThe output becomes D = A + 0 + Cin· \\n✓ If Cin=0, then D = A. Here we have a direct transfer from input A to output D. \\n✓ If Cin = 1, then D = A + 1. Here we have the value of A is incremented by 1. \\n❖ When S1So = 11, all 1' s are inserted into the Y inputs of the adder . \\n \\n✓ If Cin=0, then it produce the decrement operation D = A - 1 .This is because a number with all \\n1's is equal to the 2's comp lement of 1 (the 2's complement of binary 0001 is 1111). Adding a \\nnumber A to the 2's complement of 1 produces D = A + 2's complement of 1 = A - 1.  \\n✓ If Cin = 1, then D = A - 1 + 1 = A, which causes a direct transfer from input A to output D.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 15, 'page_label': '16'}, page_content='16 \\n \\nLogic micro operations \\n➢ Logic micro  operations specify binary operations for strings of bits stored in  registers. These \\noperations consider each bit of the register separately and treat them as binary variables.  \\n➢ For example, the exclusive -OR micro operation with  the contents of two registers R 1 and R2 is \\nsymbolized by the statement. \\n     P: R1 \\uf0df R1 \\uf0c5 R2 \\nIt specifies a logic micro operation to be executed on the individual bits of the registers provided that \\nthe control variable P = 1.  \\n➢ As a numerical example, assume that each register has four bits.  \\n✓ Let the content of R1 be 1010 \\n✓ Let the content of R2 be 1100.  \\n✓ The exclusive-OR micro operation stated above symbolizes the following logic computation: \\n1010 Content of R 1 \\n1 100 Content of R2 \\n   0110 Content of R 1 after P = 1 \\n❖ There are 16 different logic operations that can be performed with two binary variables.  \\n❖ The 16 Boolean functions o f two variables x and y are expressed in algebraic for m in the first \\ncolumn of Table I-K.  \\n     TABLE I-K:  Sixteen Logic Microoperations'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 16, 'page_label': '17'}, page_content='17 \\n \\n❖ The 16 logic micro operations are derived from these functions by replacing variable x by the binary \\ncontent of register A and variable y by the binary content of register B. \\n❖ It is important to realize that the Boolean functions listed in the first column  of Table I-K represents \\nrelationship between two binary variables x and y.  \\n❖ The logic micro operations listed in the second column  represent a relationship between the  binary \\ncontent of two registers A and B \\nHardware Implementation:  \\n➢ The hardware implementation of logic micro operations requires that logic gates be inserted for each \\nbit or pair of bits in the registers to perform the required logic function. \\n➢ Figure I-L shows one stage of a circuit that generates the four basic logic  micro operations (AND, \\nOR, XOR (exclusive-OR), and complement). It consists of four gates and a multiplexer.  \\n➢ Each of the four  logic operations is generated through a gate that performs the required logic.  The \\noutputs of the gates are applied to the data inputs of the multiplexer.  \\n➢ The two selection inputs S1 and S 0 choose one of the data inputs of the multiplexer  and direct its \\nvalue to the output. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure I-L: One stage of logic circuit'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 17, 'page_label': '18'}, page_content=\"18 \\n \\nApplications of Logic Micro operations: \\n1. Selective-set:  \\n \\nThe selective-set operation sets t o 1 the bit s in register A where there are corresponding 1's in \\nregister B. It does not affect bit positions that have 0's in B.  \\n \\nExample: \\n1010  A before \\n1100  B (logic operand)  \\n-------------------------- \\n1110  A after  \\n---------------------------  \\n• The two leftmost bits of B are 1's, so the corresponding bits of A are set to 1 . The two bits of A \\nwith corresponding 0’s in B remain unchanged. \\n• From the truth table we note that the bits of A after the operation are obtained from the logic-OR \\noperation of bits in B and previous values of A . \\n• Therefore, the OR micro operation can be used to selectively set bits of a register. \\n \\n2. Selective-complement: \\nThe selective-complement operation complements bits in A where there ar e corresponding l's in B . \\nIt does not affect bit positions that have 0's in B .  \\nExample: \\n1010                 A before  \\n1100                 B (logic operand)  \\n-------------------------------------- \\n0110                 A after \\n------------------------------------- \\n• Again the two leftmost bits of B are 1's, so the corresponding bits of A are complemented.  \\n• This example again can serve as a truth table from which one can deduce that the selective -\\ncomplement operation is just an exclusive-OR micro operation.  \\n• Therefore, the exclusive-OR micro operation can be used to selectively complement bits of a \\nregister. \\n \\n3. selective-clear \\nThe selective-clear operation clears to 0 the bits in A only where there are corresponding 1's in B.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 18, 'page_label': '19'}, page_content=\"19 \\n \\n Example: \\n1010   A before \\n1 100   B (logic operand) \\n------------------------------------------------ \\n0010   A after   \\n------------------------------------------------- \\n• Again the two leftmost bits of B are 1' s, so the corresponding bits of A are cleared to 0. One can \\ndeduce that the Boolean operation performed on the individual bits is AB' .  \\n• The corresponding logic micro operation is A \\uf0df A /\\\\  B̅    \\n \\n4. Mask operation: \\n• The mask operation is similar to the selective-clear operation except that the bits of A are cleared \\nonly where there are corresponding 0's in B.  \\n• The mask operation is an AND micro operation as seen from the following numerical \\nExample: \\n1010   A before \\n1100   B (logic operand) \\n---------------------------------- \\n1000   A after masking \\n   ----------------------------------- \\n• The two rightmost bits of A are cleared because the corresponding bits of B are 0' s. The two \\nleftmost bits are left unchanged because the corresponding bits of B are 1's.  \\n• The mask operation is more convenient  to use than the selective clear operation because most \\ncomputers provide an AND instruction, and few provide an instruction that executes the micro  \\noperation for selective-clear. \\n \\n5. Insert operation: \\n• The insert operation inserts a new value into a group of bits. This is done  by first masking the \\nbits and then ORing them with the required value. \\nFor example, suppose that an A register contains eight bits, 0110 1010.  \\n       To replace the four leftmost bits by the value 1001 we first mask the four unwanted bits:\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 19, 'page_label': '20'}, page_content=\"20 \\n \\n \\n0110 1010 A before \\n0000 1111  B (mask) \\n------------------------------------- \\n0000 1010  A after masking \\n-------------------------------------- \\n and then insert the new value: \\n \\n0000 1010  A before \\n1001 0000  B (insert) \\n---------------------------------- \\n  1001 1010       A after insertion \\n                        ----------------------------------  \\n• The mask operation is an AND micro operation and the insert operation is an OR micro operation. \\n \\n6. Clear operation: \\n• The clear operation compares the words in A and B and produces an all 0's result i f the two \\nnumbers are equal.  \\n• This operation is achieved by an exclusive-OR micro operation  as shown by the following \\nExample: \\nExample:  1010   A \\n1010   B \\n--------------------------------- \\n0000   A \\uf0df A \\uf0c5 B \\n------------------------------------- \\n \\n• When A and B are equal, the two corresponding bits are either both 0 or both 1. In either case the \\nexclusive-OR operation produces a 0.  \\n• The all-0's result is then checked to determine if the two numbers were equal.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 20, 'page_label': '21'}, page_content='21 \\n \\nShift micro operations: \\n❖ Shift m icro operations are used for serial transfer of data. They are also used  in conjunction with \\narithmetic, logic, and other data-processing operations. \\n❖ The contents of a register can be shifted to the left or the right. At the same  time that the bits are \\nshifted, the first flip-flop receives its binary information from the serial input.  \\n✓ During a shift-left operation the serial input transfers a bit into the rightmost position.  \\n✓ During a shift-right operation the serial input transfers a bit into the leftmost position.  \\n❖ The information transferred through the serial input determines the type of shift.  \\nThere are three types of shifts: logical, circular, and arithmetic. \\n1. logical shift: \\n \\n➢ A logical shift is one that transfers 0 through the serial input. We will adopt the symbols shl and shr \\nfor logical shift-left and shift-right micro operations.  \\nFor example: \\nR1 \\uf0df shl R1 \\nR2 \\uf0df shr R2 \\nare two micro operations that specify a 1-bit shift to the left of the content of register R1 and a 1-bit shift \\nto the right of the content of register R2. \\n \\n \\n \\n \\n➢ The bit transferred to the end position through the serial input is assumed to be 0 during a logical \\nshift. \\n \\n2. Circular shift: \\n \\n➢ The circular shift (also known as a rotate operation) circulates the bits of the register around the \\ntwo ends without loss of information. \\n➢ This is accomplished by connecting the serial output of the shift register to its serial input. \\n➢ We will use the symbols cil and cir for the circular shift left and right, respectively.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 21, 'page_label': '22'}, page_content='22 \\n \\n \\n \\n \\n \\n➢ The symbolic notation for the shift micro operations is shown in Table I-.M \\nTABLE I-M:  Shift Micro operations \\n \\n \\n \\n \\n \\n \\n3. Arithmetic shift: \\n \\n➢ An arithmetic shift is a micro operation that shifts a signed binary number to the left or right.  \\n✓ An arithmetic shift-left multiplies a signed binary number by 2.  \\n✓ An arithmetic shift-right divides the number by 2.  \\n➢ Arithmetic shifts must leave the sign bit unchanged because the sign of the number remains the \\nsame. \\n \\n \\n \\n \\nHardware Implementation: \\n➢ A combinational circuit shifter can be constructed with m ultiplexers as shown in Fig. I -N. The 4-bit \\nshifter has four data inputs, A0 through A3, and four data outputs, H0 through H3. \\n➢ There are two serial inputs, one for shift left (IL) and the other for shift right (IR).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 22, 'page_label': '23'}, page_content='23 \\n \\n➢ When the selection input S = 0, the input data are shifted right (down in the diagram).  \\n➢ When S = 1, the input data are shifted left (up in the diagram).  The function table in Fig. I-N shows \\nwhich input goes to each output after the shif t. A shifter with n data inputs and outputs requires n \\nmultiplexers.  \\n➢ The two serial inputs can be controlled by another multiplexer to provide the three possible types of \\nshifts. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Figure I-N: 4-bit combinational circuit shifter \\n \\n➢ When the selection input S = 0, the input data are shifted right (down in the diagram). When S = 1, \\nthe input data are shifted left (up in the diagram).  \\n➢ The function table in Fig. I-N shows which input goes to each output after the shift. A shifter with n  \\ndata inputs and outputs requires n multiplexers.  \\n➢ The two serial inputs can be controlled by another multiplexer to provide the three possible types of \\nshifts'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 23, 'page_label': '24'}, page_content='24 \\n \\nArithmetic Logic Shift Unit: \\n❖ Instead of having individual registers performing the micro  operations directly, computer \\nsystems employ a number of storage registers connected to a common  operational unit called an \\narithmetic logic unit, abbreviated ALU. \\n❖ The shift micro operations are often performed in a separate unit, but sometimes the shift u nit is \\nmade part of the overall ALU. The arithmetic logic shift unit is shown in Fig. I-O \\n \\n \\n Figure 1-O: One stage of an arithmetic logic shift unit \\n \\n❖ The subscript i designates a typical stage. Inputs Ai  and Bi are applied to both the arithmetic and \\nlogic units.  \\n❖ A particular micro  operation is selected with inputs S1 and S0. A 4 X  1 multiplexer at the output \\nchooses between an arithmetic output in Di and a logic output in Ei. \\n❖  The data in the multip lexer are selected with inputs S3 and S2. The other two data inputs to t he \\nmultiplexer receive inputs Ai - 1 for the shift-right operation and Ai+ 1 for the shift-left operation.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 24, 'page_label': '25'}, page_content=\"25 \\n \\n \\n❖ The circuit of Fig. I-O must be repeated n times for an n -bit ALU. The output carry C i + 1 of a given \\narithmetic stage must be connected to the input carry Ci of the next stage in sequence.  \\n❖ The input carry to the firs t stage is the input carry C in, which provides a selection variable for the \\narithmetic operations. \\n❖ The circuit whose one stage is specified in Fig. I -O provides eight arithmetic operation, four logic \\noperations, and two shift operations.  \\n❖ Each operation is selected with the five variabl es S3, S2, S1, S0, and Cin. The input carry Cin is used \\nfor selecting an arithmetic operation only. \\n❖ Table I-P lists the 14 operations of the ALU. The first eight are arithmetic  operations (see Table I-J) \\nand are selected with S3S 2 = 00. The next four are logic operations (see Fig. I -K) and are selected \\nwith S3S2 = 01.  \\n❖ The input carry has no effect during the logic operations and is marked with don't-care x ' s . \\n❖ The last two operations are shift operations and are selected with S 3S2 = 10 and 11 . The other three \\nselection inputs have no effect on the shift. \\n \\nTABLE I-P: Function Table for Arithmetic Logic Shift Unit\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 25, 'page_label': '26'}, page_content='26 \\n \\nInstruction Codes \\n➢ The internal organization of a digital system is defined by the sequence of micro operations it \\nperforms on data stored in its registers. \\n➢ The user of a computer can control the process by means of a program. A program is a set of \\ninstructions that specify the operations, operations operands, and the sequence by which processing \\nhas to occur. \\n \\n• A computer instruction is a binary code that specifies a sequence of micro operations for the \\ncomputer. Instruction codes together with data are stored in memory.  \\n• The computer reads each instruction from memory and places it in a control register.  \\n• The control then interprets the binary code of the instructions and proceeds to execute it by issuing \\na sequence of micro operations.  \\n• Every computer has its own unique instruction set. The ability to store and execute instructions, the \\nstored program concept, is the most important property of a general-purpose computer. \\n \\n• An instruction code is a group of bits that instruct the computer to perform a specific operation. It is \\nusually divided into parts, each having its own particular interpretation.  \\n• The most basic part of an instruction code is its operation part . The operation code of an \\ninstruction is a group of bits that define such operations as add, subtract, multiply, shift, and \\ncomplement.  \\n• The number of bits required for the operation code of an instruction depends on the total number of \\noperations available in the computer. \\n• The operation code must consists of at least n bits for a given 2” (or less) distinct operations. \\n• As an illustration, consider  a computer with 64 distinct operations, one o f them being an ADD \\noperation. The operation code consists of six bits, with a bit configuration 110010 assigned to the \\nADD operation.  \\n• When this operation code is decoded in the control unit, the computer issues control signals to read \\nan operand from memory and add the operand to a processor register. \\n• At this point  we must recognize the relationship between a computer operation and a micro \\noperation.  \\n• An operation is part of an instruction stored in computer memory . It is a binary code tells the \\ncomputer to perform a specific operation.  \\n• The control unit receives the instruction from memory and interprets the operation code bits. It then \\nissues a sequence of control signals to initiate micro operations in internal computer registers.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 26, 'page_label': '27'}, page_content='27 \\n \\n• For every operation co me, the control issues a sequence of micro operations needed for the \\nhardware implementation of the specified operation.  \\n• For this reason, an operation code is sometimes called a micro operation because it specifies a set of \\nmicro operations. \\nThe operation part of an instruction code specifies the operation to be performed.  \\n• This operation must be performed on some data stored in processor registers or in memory. \\n•  An instruction code must therefore specify not only the operation but also the re gisters or the \\nmemory words where the operands are to be found, as well as the register or memory word where \\nthe result is to be stored.  \\n• Memory words can be specified in instruction codes by their address. \\n• Processor registers can be specified by assigning to the instruction another binary code of k bits that \\nspecifies one of 2k registers. There are many variations for arranging the binary code of \\ninstructions, and each computer has its own particular instruction code format. \\n•  Instruction code formats are c onceived computer designers who specify the architecture of the \\ncomputer.  \\n \\nStored Program Organization: \\n❖ The simplest way to organize a computer is to have one processor register (Accumulator AC) and \\ninstruction code format with two parts.  \\n➢ The first part specifies the operation to be performed. \\n➢ The second specifies an address.  \\n❖ The memory address tells the control where to find an operand in memory.  \\n❖ This operand is read from memory and used as the data to be operated on together with the data \\nstored in the processor register. \\n❖ Figure 1-AA depicts this type of organization. Instructions are stored in one section of memory and \\ndata in another. \\n❖ For a memory unit with 4096 words we need 12 bits to specify an address since 212 = 4096.  \\n❖ If we store each instruction code in one 16 -bit memory word, we have available four bits for the \\noperation code (abbreviated op code) to specify one out of 16 possible operations, and 12 bits to \\nspecify the address of an operand.  \\n❖ The control reads a 16-bit instruction from the program portion of memory. It uses the 12-bit address \\npart of the instruction to read a 16-bit operand from the data portion of memory.  \\n❖ It then executes the operation specified by the operation code.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 27, 'page_label': '28'}, page_content='28 \\n \\n❖ Computers that have a single- processor register usually assign to it the name accumulator and label \\nit AC. The operation is performed with the memory operand and the content of AC. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1-AA: Stored Program Organization \\nNOTE:  \\n▪ If an operation in an instruction code does not need an operand from memory, the rest of the bits in \\nthe instruction can be used for other purposes.  \\n▪ For example, operations such as clear AC, complement AC, and increment AC operate on data \\nstored in the AC register.  \\n▪ They do not need an operand from memory. For these types of operations, the second part of the \\ninstruction code (bits 0 through 11) is not needed for specifying a memory address and can be used \\nto specify other operations for the computer. \\nIndirect Address: \\n➢ It is sometimes convenient to use the address bits of an instruction code not as an address but as the \\nactual operand. \\n➢ When the second part of an instruction code specifies an operand, the instruction is said to have an \\nimmediate operand.  \\n➢ When the second part of an instruction code specifies the address of an operand , the instruction is \\nsaid to have a direct address. \\n➢ When the second part of an instruction code specifies an address of a memory word in which the \\naddress of the operand is found. It is called indirect address. \\n➢ One bit of the instruction code can be used to distinguish between a direct and an indirect address.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 28, 'page_label': '29'}, page_content='29 \\n \\nAs an illustration, consider the instruction code format shown in Fig. 1-BB(a).  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n                                   \\n                                     (b)                                                                          (c)  \\n \\nFigure-1-BB: Demonstration of direct and indirect addresses \\n \\n• It consists of a 3 -bit operation code, 12 -bit address, and an indirect address mode bit designated \\nby I. The mode bit is 0 for a direct address and 1 for an indirect address.  \\n➢ A direct address instruction is shown in Fig. 1-B(b). It is placed in address 22 in memory.  \\n• The I bit is 0, so the instruction is recognized as a direct address instruction. The op code \\nspecifies an ADD instruction, and the address part is the binary equivalent of 457.  \\n• The control finds the operand in memory at address 457 and adds it to the content of AC.  \\n➢ A indirect address instruction is shown in Fig. 1-B(c) The instruction in address 35   \\n• The mode bit I = 1. Therefore, it is recognized as an indirect address instruction.   \\n• The address part is the binary equivalent of 300. The con trol gives to address 300 to find the \\naddress of the operand. The address of the operand in this case is 1350.  \\n• The operand found in address 1350 is then added to the content of AC.  \\n• The indirect address instruction needs two references to memory to fetch an operand.  \\n✓ The first reference is needed to read the address of the operand;  \\n✓ The second is for the operand itself.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 29, 'page_label': '30'}, page_content='30 \\n \\nComputer Registers \\n \\n➢ Computer instructions are normally stored in consecutive memory locations and are executed \\nsequentially one at a time.  \\n➢ The control reads an instruction from a specific address in memory and executes it. It then continues \\nby reading the next instruction in sequence and executes it, and so on.  \\n➢ This type of instruction sequencing needs a counter to calculate the address of the next instruction  \\nafter execution of the current instruction is completed.  \\n➢ It is also necessary to provide a register in the control unit for storing the instruction code after it is \\nread from memory.  \\n➢ The computer needs processor registers for manipulating data and a register for holding a memory \\naddress.  The registers are listed in Table 1-1 \\n \\n                                                                    Table 1-1 \\n➢ The memory unit has a capacity of 4096 words and each word contains 16 bits.  \\n➢ Twelve bits of an instruction word are needed to specify the address of an operand. This leaves three \\nbits for the operation part of the instruction and a bit to specify a direct or indirect address.  \\n• The data register (DR) holds the operand read from memory.  \\n• The accumulator (AC) register is a general purpose processing register.  \\n• The instruction read form memory is placed in the instruction register (IR).  \\n• The temporary register (TR) is used for holding temporary data during the processing. \\n• The memory address register (AR) has 12 bits since this is the width of a memory address.  \\n• The program counter (PC) also has 12 bits and it holds address of the next instruction to be read \\nfrom memory after the current the instruction is executed.  \\n \\nRegister Symbol Register Name Number of Bits Description \\nAC Accumulator 16 Processor Register \\nDR Data Register 16 Hold memory data \\nTR Temporary Register 16 Holds temporary Data \\nIR Instruction Register 16 Holds Instruction Code \\nAR Address Register 12 Holds memory address \\nPC Program Counter 12 Holds address of next instruction \\nINPR Input Register 8 Holds Input data \\nOUTR Output Register 8 Holds Output data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 30, 'page_label': '31'}, page_content='31 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Figure 1-CC:  Basic computer registers and memory. \\n• Two registers are used for input and output. The input register (INPR)  receives an 8 -bit \\ncharacter from an input device. The output register (OUTR)  holds an 8 -bit character for an \\noutput device. \\nCommon Bus System: \\n• The basic computer has eight registers, a memory unit, and a control unit. Paths must be provided to \\ntransfer information from one register to another and between memory and registers. \\n• A more efficient scheme for transferring information in a system with many registers is to use a \\ncommon bus.  \\n• It is known that how to construct a bus system using multiplexers or three-state buffer gates.  \\n• The connection of the registers and memory of the basic computer to a common bus system is \\nshown in Fig. 1-DD. \\n \\n➢ The outputs of seven registers and memory are connected to the common bus.  \\n➢ The specific output that is selected for the bus lines at any given time is determined from the binary \\nvalue of the selection variables S2 S1, and S0.  \\n➢ The number along each output shows the decimal equivalent of the required binary selection. \\n➢ For example, the number along the output of DR is 3. The 16 -bit outputs of DR are placed on the \\nbus lines when S2 S1 S0 = 011 since this is the binary value of decimal 3.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 31, 'page_label': '32'}, page_content='32 \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1-DD: Basic registers which are connected to a common bus \\n➢ The lines from the common bus are connected to the inputs of each register and the data inputs of the \\nmemory. \\n➢ The particular register whose LD (load) input is enabled receives the data from the bus during the \\nnext clock pulse transition. \\n➢ The memory receives the contents of the bus when its write input is activated. The memory places its \\n16-bit output onto the bus when the read input is activated and S2 S1 S0 = 111. \\n➢ Four registers, DR, AC, IR, and TR, have 16-bits each. Two registers, AR and PC, have 12 bits each \\nsince they hold a memory address. \\n➢ When the contents of AR or PC are applied to the 16 -bit common bus, the four most significant bits \\nare set to 0’s . When AR or PC receives informa tion from the bus, only the 12 least significant bits \\nare transferred into the register. \\n➢ The input register INPR and the output register OUTR have 8 bits each and communicate with the \\neight least significant bits (LSB) in the bus.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 32, 'page_label': '33'}, page_content='33 \\n \\n➢ INPR is connected to pro vide information to the bus but OUTR can only receive information  from \\nthe bus. This is because INPR receives a character from an input device which is then transferred to \\nAC. OUTR receives a character from AC and delivers it to an output device. \\n➢ The 16 lines of the common bus receive information from six registers and the memory unit. The bus \\nlines are connected to the inputs of six registers and the memory. \\n➢ Five registers have three control inputs: LD (load), INR (increment) and CLR (clear). The increment  \\noperation is achieved by enabling the count input of the counter. Two registers have only a LD input. \\n➢ The input data and output data of the memory are connected to the common bus, but the memory \\naddress is connected to AR. Therefore, AR must always be used to specify a memory address. \\n➢ The content of any register  can be specified for the memory data input during a write operation. \\nSimilarly, any register can receive the data from memory after a read operation except AC. \\n➢ The 16 inputs of AC  come from an adder and logic circuit. This circuit has three sets of inputs. One \\nset of 16 -bit inputs come from the outputs of AC. They are used to implement register micro -\\noperations such as complement AC and shift AC. \\n➢ Another set of 16 -bit inputs come from the data regis ter DR. The inputs from DR and AC are used \\nfor arithmetic and logic micro-operations, such as add DR to AC or and DR to AC. \\n➢ The result of an addition is transferred to AC and the end carry -out of the addition is transferred to \\nflip-flop E (extended AC bit). A third set of 8-bit inputs come from the input register INPR. \\n❖ Note that the content of any register can be applied onto the bus and an operation can be \\nperformed in the adder and logic circuit during the same clock cycle.  \\n❖ The clock transition at the en d of the cycle transfers the content of the bus into the designated \\ndestination register and the output of the adder and logic circuit into AC. \\n❖  For example, the two micro -operations:  DR ← AC and AC ← DR can be executed at the \\nsame time. \\n❖ This can be done by placing the content of AC on the bus (with S2S1S0 = 100), enabling the \\nLD(load) input of DR, transferring the content of DR through the adder and logic circuit into \\nAC, and enabling the LD (load) input of AC, all during the same clock cycle. The two tra nsfers \\noccur upon the arrival of the clock pulse transition at the end of the clock cycle.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 33, 'page_label': '34'}, page_content='34 \\n \\nComputer Instructions \\n \\n➢ The basic computer has three instruction code formats. Each format has 16 bits. The operation code \\n(opcode) part of the instruction contains three bits  and the remaining 13 bits depends on the \\noperation code encountered. \\n➢ A memory reference instruction  uses 12 bits to specify an address and one bit to specify the \\naddressing mode I. I is equal to 0 for direct address and to 1 for indirect address. \\n➢ The register-reference instructions  are recognized by the operation code 111 with a 0 in the \\nleftmost bit (bit 15) of the instruction.  \\n➢ A register-reference instruction specifies an operation on or a test of the AC register. An operand \\nfrom memory is not needed; therefore, the other 12 bits are used to specify the operation or test to be \\nexecuted. \\n➢ An input -output instruction  does not need a reference to memory and is recognized by the \\noperation code 111 with a  1 in the leftmost bit  of the instruction. The remaining 12 bits are used to \\nspecify the type of input-output operation or test performed. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1-E:  Basic computer instruction formats. \\n➢ The type of instruction is recognized by the computer control from the four bits in positions 12 \\nthrough 15 of the instruction.  \\n➢ If the three op code bits in positions 12 through 14 are not equal to 111 , the instruction is a \\nmemory-reference type and the bit in position 15 is taken as the addressing mode I.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 34, 'page_label': '35'}, page_content='35 \\n \\n➢ If the three op code bits in positions 12 through 14 are equal to 111  , control then inspects the bit in \\nposition 15. If this bit is 0, the instruction is a register -reference type. If the bit is 1, the  instruction \\nis an input-output type. \\n \\nNOTE: Note that the bit in position 15 of the instruction code is designated by the symbol I but is not  \\n   used as a mode bit when the operation code is equal to 111. \\n \\n❖ Only three bits of the instruction are used for the operation code. It may seem that the computer is \\nrestricted to a maximum of eight distinct operations . However, since register reference and input -\\noutput instructions use the remaining 12 bits as part of the operation code, the total number of \\ninstruction chosen for the basic computer is equal to 25. \\n \\nTABLE 1-2:  Basic Computer Instructions  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n➢ The instructions for the computer are listed in Table 1 -2. The symbol designation is a three letter \\nword and represents an abbreviation intended for programmers and users. The hexadecimal code is \\nequal to the equivalent hexadecimal number of the binary code used for the instruction. \\n➢ A memory-reference instruction  has an address part of 12 bits. The address part is denoted by \\nthree x’s and stand for the three hexadecimal digits corresponding to the 12-bit address. \\n➢ The last bit of the instruction is designated by the symbol I.  \\nMemory \\nreference \\ninstructions \\nRegister-\\nreference \\ninstructions \\nInput-output \\ninstructions'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 35, 'page_label': '36'}, page_content='36 \\n \\n➢ When I = 0, the last four bits of an instruction have a hexadecimal digit equivalent from 0 to 6 since \\nthe last bit is 0. When I = 1 , the hexadecimal digit equivalent of the last four bits of the instruction \\nranges from 8 to E since the last bit is I. \\n➢ Register-reference instructions  use 16 bits to specify an operation. The leftmost four bits are \\nalways 0111, which is equivalent to hexadecimal 7. The other three hexadecimal digits give the \\nbinary equivalent of the remaining 12 bits.  \\n➢ The input-output instructions  also use all 16 bits to specify an operation. The last four bits are \\nalways 1111, equivalent to hexadecimal F. \\n \\nTiming and Control \\n→ The timing for all registers in the basic computer is controlled by a master clock generator.  \\n→ The clock pulses do not change the state of a register unless the register is enabled by a control \\nsignal.  \\n→ The control signals are generated in the control uni t and provide control inputs for the multiplexers \\nin the common bus, control inputs in processor registers, and micro operations or the accumulator. \\n→ There are two major types of control organization:  \\n                               Hardwired control and micro programmed control. \\n→ In the hardwired organization , the control logic is implemented with gates, flip -flops, decoders, \\nand other digital circuits. It has the advantage that it can be optimized to pr oduce a fast mode of \\noperation.  \\n→ In the micro programmed organization , the control information is stored in a control memory. \\nThe control memory is programmed to initiate the required sequence of micro operations. \\n \\nBlock diagram of control unit: \\n \\n➢ The block diagram of the control unit is shown in Fig. 1 -FF. It consists of two decoders, a sequence \\ncounter, and a number of control logic gates. \\n➢ An instruction read from memory is placed in the instruction register (IR ): .The instruction re gister \\nis shown again in Fig. 1-FF where it is divided into three parts:  \\nThe I bit, the operation code, and bits 0 through 11.  \\n➢ The operation code in bits 12 through 14 are decoded with a 3 × 8 decoder. The eight outputs of the \\ndecoder are designated by the symbols D0 through D7. The subscripted decimal number is \\nequivalent to the binary value of the corresponding operation code.  \\n➢ Bits 15 of the instruction is transferred to a flip -flop designated by the symbol I.  Bits 0 through 11 \\nare applied to the control logic gates.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 36, 'page_label': '37'}, page_content='37 \\n \\n➢ The 4-bit sequence counter can count in binary from 0 through 15. The outputs of the counter are \\ndecoded into 16 timing signals T0 through T15.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1-FF:  Control unit basic computer \\n➢ The sequence counter  SC can be incremented or cleared synchronously. Most of the time, the \\ncounter is incremented to provide the sequence of timing signals out of the 4 × 16 decoder. Once in a \\nwhile, the counter is cleared to 0, causing the next active timing signal to be T0. \\n➢ As an example, consider the case where SC is incremented to provide timing signals T0, T1, T2, T3, \\nand T4 in sequence. At time T 4, SC is cleared to 0 if decoder output D3 is active. This is expressed \\nsymbolically by the statement: \\nD3 T4: SC ← 0 \\nThe Timing diagram: \\n➢ The timing diagram of Fig. 1 -GG shows the time relationship of the control signals. The sequence \\ncounter SC responds to the positive transition of the clock. \\n➢  Initially, the CLR input of SC is active. The first positive transition of the Clock clears SC to 0, \\nwhich in turn activates the timing signal T0 out of the decoder. \\n➢  T0 is active during one clock cycle. The positive clock transition labeled T 0 in the diagram will \\ntrigger only those registers whose control inputs are connected to timing signal T0.  \\n➢ SC is incremented with every positive clock transition, unless its CLR input is active. This produces \\nthe sequence of timing signals T 0, T 1, T 2, T 3, T 4, and so on, as shown in the diagram (Not e the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 37, 'page_label': '38'}, page_content='38 \\n \\nrelationship between the timing signal and its corresponding positive clock transition.) If SC is not \\ncleared, the timing signals will continue with T5, T6, up to T15 and back to T0. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1-GG:  Example of control timing signals \\n \\n➢ The last three waveforms in Fig 1 -GG show how SC is cleared when D 3T4 = 1. Output D 3 from the \\noperation decoder becomes active at the end of timing signal T2.  \\n➢ When timing signal T 4 becomes active, the output of the AND gate that implements the control \\nfunction D3D4 becomes active.  \\n➢ This signal is applied to the CLR input of SC. On the next positive clock transition (the one marked \\nT4 in the diagram) the counter is cleared to 0.  \\n➢ This causes the timing signal T 0 to become active instead of T 5 that would hav e been active if SC \\nwere incremented instead of cleared.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 38, 'page_label': '39'}, page_content='39 \\n \\nInstruction Cycle: \\n• In the basic computer each instruction cycle consists or the following phases: \\n \\n1) Fetch an instruction from memory. \\n2) Decode the instruction \\n3) Read the effective address from memory if the instruction has an indirect address. \\n4) Execute the instruction. \\n \\n❖ Upon the completion of step 4, the control goes back to step 1 to fetch, decode, and execute the next \\ninstruction. This process continues indefinitely unless a HALT instruction is encountered. \\n \\nFetch and Decode: \\n• Initially, the program counter PC is loaded with the address of the first instruction in the program. \\n• The sequence counter SC is cleared to 0, providing a decoded timing signal T0.  \\n• After each clock pulse, SC is incremented  by one so that the timing signals go through a sequence \\nT0, T1, T2, and so on.  \\n• The micro operations for the fetch and decode phases can be specified by the following register \\ntransfer statements. \\n \\nT0: AR ← PC \\nT1: IR ← M[AR], PC ← PC + 1 \\nT2: D0… D1 ← Decode IR (12-14), AR ← IR (0 −11), 1 ← IR (15) \\n• Since only AR is connected to the address inputs of memory, it is necessary to transfer the address \\nfrom PC to AR during the clock transition associated with timing signal T0. \\n• The instruction rea d from memory is then placed in the instruction register IR with the clock \\ntransition associated with timing signal T1. \\n• At the same time, PC is incremented by one to prepare it for the address of the next instruction in the \\nprogram.  \\n• At time T2, the operat ion code in IR is decoded, the indirect bit is transferred to flip -flop I, and the \\naddress part of the instruction is transferred to AR.  \\n• Note that SC is incremented after each clock pulse to produce the sequence T0, T1, and T2. \\n \\nFigure 1-HH shows how the first two register transfer statements are implemented in the bus system. To \\nprovide the data path for the transfer of PC to AR we must apply timing signal T0 to achieve the \\nfollowing connection:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 39, 'page_label': '40'}, page_content='40 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Figure 1-HH: Register transfers for the fetch phase \\n1. Place the content of PC onto the bus by making the bus selection inputs S2S1S0 equal to 010. \\n2. Transfer the content of the bus to AR by enabling the LD input of AR. \\n \\n• The next clock transition initiates the transfer form PC to AR since T0 = 1. In order to implement the \\nsecond statement \\n T1: IR ← M [AR], PC ← PC + 1 \\n• It is necessary to use timing signal T1 to provide the following connections in the bus system. \\na) Enable the read input of memory. \\nb) Place the content of memory onto the bus by making S2S1S0 = 111. \\nc) Transfer the content of the bus to IR by enabling the LD input of IR. \\nd) Increment PC by enabling the INR input of PC.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 40, 'page_label': '41'}, page_content='41 \\n \\nThe next clock transition initiates the read and increment operations since T1 = 1. \\nFigure 1 -HH duplicates a portion of the bus system and shows how T0 and T1 are connected to the \\ncontrol inputs of the registers, the memory, and the bus selection inputs. \\n \\nDetermine the Type of Instruction: \\n➢ Decoder output D7 is equal to 1 if the operation code is equal to binary 111. We determine that if  \\nD7 =1, the instruction must be a register-reference or input-output type.  \\n➢ If D7 = 0, the operation code must be one of the other seven values 000 through 110, specifying \\nmemory reference instruction.  \\n➢ Control then inspects the value of the first bit of the instruction, which is now available in flip-flop I.  \\n➢ If D 7 = 0 and I = 1, we have a memory -reference instruction with an indirect address. It is then \\nnecessary to read the effective address from memory.  \\n➢ The micro o peration for the indirect address condition can be symbolized by the register transfer \\nstatement \\nAR ← M [AR] \\n➢ Initially, AR holds the address part of the instruction. This address is used during the memory read \\noperation. The word at the address given by AR is read from memory and placed on the common \\nbus. The LD input of AR is then enabled to receive the indirect address that resided in the 12 least \\nsignificant bits of the memory word. \\n \\n❖ The three instruction types are subdivided into four separate paths. The selected operation is \\nactivated with the clock transition associated with timing signal T3. This can be symbolized as \\nfollows: \\n \\nD7 = 0  I = 1  D’7 IT3: AR← M [AR] \\n   0            0  D’7 I’T3; Nothing \\n   1      0  D7 I’ T3: Execute a register-reference instruction \\n  1      1  D7 IT3: Execute an input-output instruction \\n \\n❖ The flowchart of Fig. 1 -II presents an initial configuration for the instruction cycle and shows how \\nthe control determines the instruction type after the decoding. \\n \\nNote that the sequence counter SC is either incremented or cleared to 0 with every positive clock \\ntransition. We will adopt the convention that if SC is incremented, we will not write the statement SC ← \\nSC + 1, but it will be implied t hat the control goes to the next timing signal in sequence. When SC is to \\nbe cleared, we will include the statement Sc ← 0.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 41, 'page_label': '42'}, page_content='42 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1-II : Flowchart for instruction cycle (initial configuration). \\n \\nRegister-Reference Instructions: \\n• Register-reference instructions are recognized by the control when D7 = 1 and I = 0.  \\n• These instructions use bits 0 through 11 of the instruction code to specify one of 12 instructions.  \\n• These 12 bits are available in IR (0-11). They were also transferred to AR during time T2. \\nThe control functions and micro operations for the register-reference instructions are listed in Table 1- 3'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 42, 'page_label': '43'}, page_content='43 \\n \\n• Each control function needs the Boolean relation D7 I’ T3, which we designate for convenience by \\nthe symbol r. The control function is distinguished by one of the bits in IR (0- 11).  \\n• By assigning the symbol Bi to bit i of IR, all control functions can be simply denoted by rBi. \\n• For example, the instruction CLA has the hexadecimal code 7800 (see Table 3.1), which gives the \\nbinary equivalent 0111 1000 0000 0000.  \\n• The first bit is a zero and is equivalent to I’. The next three bits constitute the operation code and are \\nrecognized from decoder output D7. Bit 11 in IR is 1 and is recognized from B11. The control \\nfunction that initiates the micro operation for this instruction is D7 I’ T3 B11 = rB11. \\n• The first seven register -reference instructions  perform clear, complement, circular shift, and \\nincrement micro operations on the AC or E registers.  \\n• The next four instructions cause a skip of the next instruction in sequence when a stated condition is \\nsatisfied.  \\n• The HLT instruction clears a start -stop flip-flops S and stops the sequence counter from counting. \\nTo restore the operation of the computer, the start-stop flip-flop must be set manually. \\n \\nMemory-Reference Instructions: \\n• Table 1-4 lists the seven memory-reference instructions. The decoded output Di for i = 0, 1, 2, 3, 4, \\n5, and 6 from the operation decoder that belongs to each instruction is included in the table. \\n• The effective address of the instruction is in the address register AR and was placed there during \\ntiming signal T2 when I = 0, or during timing signal T3 when I = 1. \\n• The executio n of the memory -reference instructions starts with timing signal T4  the symbolic \\ndescription of each instruction is specified in the table in terms of register transfer notation.  \\n• The actual execution of the instruction in the bus system will require a sequence of micro operations. \\nThis is because data stored in memory cannot be processed directly.  \\n• The data must be read from memory to a register where they can be operated on with logic circuits. \\n \\nTable 1-4: Memory-Reference Instructions'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 43, 'page_label': '44'}, page_content='44 \\n \\n1) AND to AC \\nThis is an instruction that performs the AND logic operation  on pairs of bits in AC and the memory \\nword specified by the effective address. The result of the operation is transferred to AC. The micro \\noperations that execute this instruction are: \\nD0T4 : DR ← M [AR] \\nD0T5: AC ← AC Λ DR, SC ← 0 \\n• Two timing signals are needed to execute the instruction. The clock transition associated with timing \\nsignal T4 transfers the operand from memory into DR.  \\n• The clock transition associated with the next timing signal T5 transfers to AC the result of the AND \\nlogic operation between the contents of DR and AC.  \\n• The same clock transition clears SC to 0, transferring control to timing signal T0 to start a new \\ninstruction cycle. \\n2) ADD to AC \\nThe instruction adds the content of the memory word specified by the effective address to the value of \\nAC. The sum is transferred into AC and the output carry C out is transferred to the E (extended \\naccumulator) flip-flop. The micro operations needed to execute this instruction are: \\nD1T4: DR ← M [AR] \\nD1T5: AC ← AC + DR, E ← Cout, SC ← 0 \\nThe same two timing signals, T4 and T5, are used again but with operation decoder D1 instead of D0.  \\n3) LDA : Load to AC \\nThis instruction transfers the memory word specified by the effective address to AC. The micro \\noperations needed to execute this instruction are: \\nD2T4: DR ← M [AR] \\nD2T5: AC ← DR, SC ← 0 \\n• From the bus diagram we note that there is no direct path from the bus into AC.  \\n• The adder and logic circuit receive information from DR which can be transferred into AC.  \\n• Therefore, it is necessary to read the memory word into D R first and then transfer the content of DR \\ninto AC. \\n4) STA : Store AC \\nThis instruction stores the content of AC into the memory word specified by the effective address. \\nSince the output of AC is applied to the bus and the data input of memory is connected to the bus, \\nwe can execute this instruction with one micro operation: \\nD3T4: M [AR] ← AC, SC ← 0'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 44, 'page_label': '45'}, page_content='45 \\n \\n5) BUN : Branch Unconditionally \\n• This instruction transfers the program to the instruction specified by the effective address. \\n• Remember that PC holds the address of the instruction to be read from memory in the next \\ninstruction cycle.   \\n• The BUN instruction allows the programmer to specify an instruction out of sequence and we say \\nthat the program branches (or jumps) unconditionally.  \\n• The instruction is executed with one micro operation: \\nD4T4: PC ← AR, SC ← 0 \\n• The effective address from AR is transferred through the common bus to PC. \\n• Resetting SC to 0 transfers control to T0. The next instruction is then fetched and executed from the  \\nmemory address given by the new value in PC. \\n6) BSA : Branch and Save Return Address \\n• This instruction is useful for branching to a portion of the program called a subroutine or procedure. \\nWhen executed, the BSA instruction stores the address of the next inst ruction in sequence (which is \\navailable in PC) into a memory location specified by the effective address. \\n• The effective address plus one is then transferred to PC to serve as the address of the first instruction \\nin the subordinate. This operation was specified in Table 1-5 with the following register transfer: \\nM [AR] ← PC, PC ← AR + 1 \\n❖ An Example: Demonstrates how BSA instruction is used with a subordinate is shown in  \\nFig. 1-J. The BSA instruction is to be in memory at address 20. The I bit is 0 and the address part of \\nthe instruction has the binary equivalent of 135.  \\n❖ After the fetch and decode phases, PC contains 21, which is the address of the next instruction in the \\nprogram (referred to as the return address). \\n❖ AR holds the effective address 135. This is shown in part (a) of the figure. The BSA instruction \\nperforms the following numerical operation: \\nM[135] ← 21, PC ← 135 + 1 = 136 \\n❖ The result of this operation is shown in part (b) of the figure. The return address 21 is stored in \\nmemory location 135 and control continues with the subordinate program starting from address 136. \\n❖ The return to the original program (at address 21) is accomplished by means o f an indirect \\nBUN instruction placed at the end of the subroutine.  \\n❖ When this instruction is executed, control goes to the indirect phase to read the effective address at \\nlocation 135, where it finds the previously saved address 21.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 45, 'page_label': '46'}, page_content='46 \\n \\n❖ When the BUN instruction is executed, the effective address 21 is transferred to PC.  \\n❖ The next instruction cycle finds PC with the value 21, so control continues to execute the instruction \\nat the return address. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1-JJ: Example of BSA instruction execution. \\n \\n❖ It is not possible to perform the operation of the BSA instruction in one clock cycle when we use the \\nbus system of the basic computer.  \\n❖ To use the memory and the bus properly, the BSA instruction must be executed with a sequence of \\ntwo micro operations: \\n                             D5T4: M[AR] ← PC, AR ← AR + 1 \\n                              D5T5: PC ← AR, SC ← 0 \\n❖ Timing signal T4 initiates a memory write operation, places the content of PC onto the bus, and \\nenables the INR input of AR.  \\n❖ The memory write operation is completed and AR is incremented by the time the next clock \\ntransition occurs. The bus is used at T5 to transfer the content of AR to PC. \\n \\n \\n7) ISZ : Increment and Skip if Zero \\n❖ This instruction increment the word specified by the effective address, and if the incremented value \\nis equal to 0, PC is incremented by 1.  \\n❖ The programmer usually stores a negative number (in 2’s complement) in the memory word. As \\nthis negative number is repeatedly incremented by one, it eventually reaches the value of zero.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 46, 'page_label': '47'}, page_content='47 \\n \\n❖ At that time PC is incremented by one in order to skip the next instruction in the program. \\n❖ Since it is not possible to increment a word inside the memory, it is necessary to read the  word into \\nDR, increment DR, and store the word back into memory.  \\n❖ This is done with the following sequence of micro operations: \\nD6T4: DR ← M[AR] \\nD6T5: DR ← DR + 1 \\nD6T6 : M [AR] ← DR, if (DR = 0) then (PC ← PC + 1), SC ← 0 \\n \\nCONTROL FLOWCHART: \\n \\nA flowchart showing all micro operations for the execution of the seven memory -reference instructions \\nis shown in Fig 1-KK. The control functions are indicated on top of each box. The micro operations that \\nare performed during time T4, T5, or T6 depend on the operation code value. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1-KK: Control flowchart for memory-reference instructions'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 47, 'page_label': '48'}, page_content='48 \\n \\nInput-Output and Interrupt: \\nCommercial computers include many types of input and output devices. To demonstrate the most basic \\nrequirements for input and output communication. \\nInput-Output Configuration \\n \\n• The terminal sends and receives serial information. Each quantity of information has eight bits of an \\nalphanumeric code.  \\n• The transmitter interface receives serial information from the keyboard and transmits it to INPR.  \\n• The receiver interface receives information from OUTR and sends it to the printer serially. \\n• The input register  INPR consists of eight bits and holds alphanumeric input information. The 1 -bit \\ninput flag FGI is a control flip -flop. The flag bit is set to 1 when new Information is available in the \\ninput device and is cleared to 0 when the information is accepted by the computer. \\n \\n \\nFigure 1-LL: Input Output Configuration \\n \\n \\n \\nThe process of information transfer is as follows. \\n \\n➢ Initially, the input flag FGI is cleared to 0.  \\n➢ When a key is struck in the keyboard, an 8 -bit alphanumeric code is shifted into INPR and the \\ninput flag FGI is set to 1.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 48, 'page_label': '49'}, page_content='49 \\n \\n➢ The computer checks the flag bit; if it is 1, the information from INPR is transferre d in parallel \\ninto AC and FGI is cleared to 0.  \\n➢ Once the flag is cleared, new information can be shifted into INPR by striking another key. \\n  \\n    The process of information receiver is as follows. \\n \\n➢ Initially, the output flag FGO is set to 1. The computer checks the flag bit, if it is 1, the \\ninformation from AC is transferred in parallel to OUTR and FGO is cleared to 0.  \\n➢ The output device accepts the coded information, prints the corresponding charact er, and when \\nthe operation is completed, it sets FGO to the computer does not load a new character into \\nOUTR when FGO is 0 because this condition indicates that the output device is in the process of \\nprinting the character. \\n \\nInput Output Instructions: \\n• Input and output instructions are needed for transferring information to and from AC register , for \\nchecking the flag bits, and for controlling the interrupt facility. \\n• Input-output instructions have an operation code 1111 and are recognized by the control when  \\nD7 = and I = 1.  \\n• The remaining bits of the instruction specify the particular operation. The control functions and \\nmicro operations for the input-output instructions are listed in Table 1-5.  \\n• These instructions are executed with the clock transition associated with timing signal T3.  \\n• Each control function needs a Boolean relation D 7IT3, which we designate for convenience by the \\nsymbol p.  \\n• The control function is distinguished by one of the bits in IR (6 -11). By assigning the symbol B i to \\nbit i of IR, all control functions can be denoted by pB i for i = 6 though 11. The sequence counter SC \\nis cleared to 0 when p = D7IT3 =1. \\n \\nTable 1-5: Input-Output Instructions \\n \\nD7 IT3 = p (common to all input-output instructions) \\nIR (i) = Bi [bit in IR (6−11) that specifies the instruction]'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 49, 'page_label': '50'}, page_content='50 \\n \\n❖ INP: The INP instruction transfers the input information from INPR into the eight low -order bits of \\nAC and also clears the input flag to 0.  \\n❖ OUT: The OUT instruction transfers the eight least significant bits of AC into the ou tput register \\nOUTR and clears the output flag to 0.  \\n❖ SKI and SKO: These two instructions check the status of the flags and cause a skip of the next \\ninstruction if the flag is 1.  \\nThe instruction that is skipped will normally be a branch instruction to retu rn and check the flag \\nagain. The branch instruction is not skipped if the flag is 0. If the flag is 1, the branch instruction is \\nskipped and an input or output instruction is executed.  \\n❖ ION and IOF: These two instructions set and clear an interrupt enable flip-flop IEN. The purpose of \\nIEN is explained in conjunction with the interrupt operation. \\n \\nThe interrupt cycle:  \\n❖ The way that the interrupt is handled by the computer can be explained by means of the \\nflowchart of Fig. 1-MM.  \\n❖ An interrupt flip-flop R is included in the computer. When R = 0, the computer goes through an \\ninstruction cycle. \\n❖ During the execute phase of the instruction cycle IEN is checked by the control. If it is 0 , it \\nindicates that the programmer does not want to use the inte rrupt, so control continues with the \\nnext instruction cycle.  \\n❖ If IEN is 1, control checks the flag bits. If both flags are 0, it indicates that neither the input nor \\nthe output registers are ready for transfer of information. In this case, control continue s with the \\nnext instruction cycle.  \\n❖ If either flag is set to 1 while IEN = 1, flip-flop R is set to 1.  \\n❖ At the end of the execute phase, control checks the value of R, and if it is equal to 1, it goes to an \\ninterrupt cycle instead of an instruction cycle. \\n• The interrupt cycle is a hardware implementation of a branch and save return address operation.  \\n• The return address available in PC is stored in a specific location where it can be found later when \\nthe program returns to the instruction at which it was interrupted. \\n• This location may be a processor register, a memory stack, or a specific memory location.  \\n• Here we choose the memory location at address 0 as the place for storing the return address. \\n• Control then inserts address 1 into PC and clears IEN and R so that no more interruptions can occur \\nuntil the interrupt request from the flag has been serviced.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 50, 'page_label': '51'}, page_content='51 \\n \\n \\n Figure 1-MM: Flowchart for interrupt cycle \\n• An example that shows what happens during the interrupt cycle is shown in Fig 1-NN. \\n• Suppose that an interrupt occurs and R is set to 1 while the control is executing the instruction at \\naddress 255. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1-NN: Demonstration of the interrupt cycle'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 51, 'page_label': '52'}, page_content='52 \\n \\n• At this time, the return address 256 is in PC. \\n• The programmer has previously placed an input -output service program in memory starting from \\naddress 1120 and a BUN 1120 instruction at address 1. This is shown in Fig. 1-NN (a). \\n• When control reaches timing signal T0 and finds that R = 1, it proceeds with the interrupt cycle.  \\n• The content of PC (256) is stored in memory location 0, PC is set to 1, and R is cleared to 0. At the \\nbeginning of the next instruction cycle, the instruction that is read from memory i s in address 1 since \\nthis is the content of PC.  \\n• The branch instruction  (BUN) at address 1 causes the program to transfer to the input -output \\nservice program at address 1120.  \\n• This program checks the flags, determines which flag is set, and then transfers the required input or \\noutput information.  \\n• Ones this is done, the instruction ION is executed to set IEN to 1 (to enable further interrupts), and \\nthe program returns to the location where it was interrupted. This is shown in Fig. 1-NN (b). \\n• The instruction that returns the computer to the original place in the main program is a branch \\nindirect instruction with an address part of 0.  \\n• This instruction is placed at the end of the I/O service program. After this instruction is read from \\nmemory during the fetch p hase, control goes to the indirect phase (because I = 1) to read the \\neffective address. The effective address is in location 0 and is the return address that was stored there \\nduring the previous interrupt cycle.   \\n• The execution of the indirect BUN instruct ion results in placing into PC the return address from \\nlocation 0. \\nComplete Computer Description (Instructions): \\nThe final flowchart of the instruction cycles (including the interrupt cycle) for the basic computer, is \\nshown in Fig. 1-OO.  \\n• The interrupt flip-flop R may be set at any time during the indirect or execute phases.  \\n• Control returns to timing signal T 0 after SC is cleared to 0. If R = 1, the computer goes through an \\ninterrupt cycle. If R = 0, the computer goes through an instruction cycle. \\n• If the instruction is one of the memory-reference instructions , to execute the decoded instruction \\naccording to the flowchart of Fig. 1-KK.  \\n• If the instruction is one of the register-reference ins tructions, it is executed with one of the micro \\noperations listed in Table 1-3. \\n• If it is an input-Output instruction, it is executed with one of the micro operations listed in Table 1-5.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Office 365', 'creator': 'Microsoft® Word for Office 365', 'creationdate': '2019-07-30T07:59:37+00:00', 'moddate': '2019-07-30T07:59:37+00:00', 'source': 'COA UNIT-I.pdf', 'total_pages': 53, 'page': 52, 'page_label': '53'}, page_content='53 \\n \\n➢ We now modify the fetch and decode phases of the instruction cycle. Instead of using only \\ntiming signals T0, T1, and T2 .  \\n➢ We will AND the three timing signals with R’  so that the fetch and decode phases will be \\nrecognized from the three control functions R′ T0, R′ T1, and R′ T2.  \\n➢ The reason for this is that after the instruction is executed and SC is cleared to 0, the control will \\ngo through a fetch phase only if R = 0. Otherwise, if R = I, the control will go through an \\ninterrupt cycle.  \\n➢ The interrupt cycle stores the return addre ss (available in PC) into memory location 0, branches \\nto memory location 1, and clears IEN, R, and SC to 0.  \\n➢ This can be done with the following sequence of micro operations. \\nRT0: AR ← 0, TR ← PC \\nRT1: M[AR] ← TR, PC ← 0 \\nRT2: PC ← PC + 1, IEN ← 0, R ← 0, SC ← 0. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 1-OO:  Flowchart for computer operation.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### reading an PDF file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('COA UNIT-I.pdf')\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c76ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40958a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview'}, page_content='')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader1=WebBaseLoader(web_paths=('https://docs.langchain.com/oss/python/langchain/overview',),\n",
    "                      bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                          class_=('post-title','post-content','post-header')\n",
    "                      )))\n",
    "docs=loader1.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf60d6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs=ArxivLoader(query=\"1706.03762\",load_max_docs=2).load()\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f74841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (Generative AI or GenAI) is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data in response to input, which often comes in the form of natural language prompts.\\nThe prevalence of generative AI tools has increased significantly since the AI boom in the 2020s. This boom was made possible by improvements in deep neural networks, particularly large language models (LLMs), which are based on the transformer architecture. Major tools include LLM-based chatbots such as ChatGPT, Claude, Copilot, DeepSeek, Google Gemini and Grok; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTX and Sora. Technology companies developing generative AI include Alibaba, Anthropic, Baidu, DeepSeek, Google, Lightricks, Meta AI, Microsoft, Mistral AI, OpenAI, Perplexity AI, xAI, and Yandex.\\nGenerative AI has been adopted in a variety of sectors, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, and product design.  \\nGenerative AI has been used for cybercrime, and to deceive and manipulate people through fake news and deepfakes. Generative AI may lead to mass replacement of human jobs. The tools themselves have been described as violating intellectual property laws, since they are trained on copyrighted works. Many generative AI systems use large-scale data centers whose environmental impacts include e-waste, consumption of fresh water for cooling, and high energy consumption that is estimated to be growing steadily. Generative AI continues to evolve rapidly as new models and applications emerge.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (Generative AI or GenAI) is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data in response to input, which often comes in the form of natural language prompts.\\nThe prevalence of generative AI tools has increased significantly since the AI boom in the 2020s. This boom was made possible by improvements in deep neural networks, particularly large language models (LLMs), which are based on the transformer architecture. Major tools include LLM-based chatbots such as ChatGPT, Claude, Copilot, DeepSeek, Google Gemini and Grok; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTX and Sora. Technology companies developing generative AI include Alibaba, Anthropic, Baidu, DeepSeek, Google, Lightricks, Meta AI, Microsoft, Mistral AI, OpenAI, Perplexity AI, xAI, and Yandex.\\nGenerative AI has been adopted in a variety of sectors, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, and product design.  \\nGenerative AI has been used for cybercrime, and to deceive and manipulate people through fake news and deepfakes. Generative AI may lead to mass replacement of human jobs. The tools themselves have been described as violating intellectual property laws, since they are trained on copyrighted works. Many generative AI systems use large-scale data centers whose environmental impacts include e-waste, consumption of fresh water for cooling, and high energy consumption that is estimated to be growing steadily. Generative AI continues to evolve rapidly as new models and applications emerge.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nThe origins of algorithmically generated media can be traced to the development of the Markov chain, which has been used to model natural language since the early 20th century. Russian mathematician Andrey Markov introduced the concept in 1906, including an analysis of vowel and consonant patterns in Eugeny Onegin. Once trained on a text corpus, a Markov chain can generate probabilistic text.\\nBy the early 1970s, artists began using computers to extend generative techniques beyond Markov models. Harold Cohen developed and exhibited works produced by AARON, a pioneering computer program designed to autonomously create paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural networks (Late 2000s-) ===\\n\\nMachine learning uses both discriminative models and generative models to predict data. Beginning in the late 2000s, the introduction of deep learning technology led to improvements in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images, such as DeepDream.\\nIn 2017, the Transformer network enabled advancements in generative mo'),\n",
       " Document(metadata={'title': 'Generative AI pornography', 'summary': 'Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including generative adversarial networks (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.', 'source': 'https://en.wikipedia.org/wiki/Generative_AI_pornography'}, page_content=\"Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including generative adversarial networks (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n\\n== Functions and production strategies ==\\nAI pornography platforms, beyond account creation and social media linking, primarily enable users to generate sexual images through feature selection or text prompting. Users can customize bodies, clothing, and sociodemographic traits, and browse categorized galleries of user‑generated content. Several sites also support short pornographic videos or GIFs and modification tools such as nudifiers, deepfakes, and facemorphing. Platforms often allow fine‑tuning of parameters such as settings, style, or theme, and provide prompt enhancers or suggestions to improve outputs. Users may edit generated images, refine prior prompts, modify others’ work, or upload personal material as a basis, with iterative and collaborative content creation. Some websites additionally host interactive “erobots,” customizable in real time for appearance, personality, memories, speech, and profession, enabling tailored sexual and non‑sexual interactions. Less common features include VR integration, AI porn games, audio or doodle prompts, and consensual replication of individuals with verification.\\n\\n\\n== History ==\\nThe use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI's release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI's warnings against sexual imagery, SD's public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\\n\\n\\n=== AI-generated influencers ===\\nOne application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but convincing experience. While popular among niche audiences, these virtual influencers have prompted discussions about authenticity, consent, and the blurring line between human and AI-generated content, especially in adult entertainment.\\n\\n\\n=== The growth of AI porn sites ===\\nBy 2023, websites dedicated to AI-generated adult content had gained traction, catering to audiences seeking customizable experiences. These platforms allow users to create or view AI-generated pornography tailored to their preferences. These platforms enable users to create or view AI-generated adult content appealing to different preferences through prompts and tags, customizing body type, facial features, and art styles. Tags further refine the output, creating niche and diverse content. Many sites feature extensive image libraries and continuous content feeds, combining personalization with discovery and enhancing user engagement. AI porn sites, therefore, attract those seeking unique or niche experiences, sparking debates on creativity and the ethical boundaries of AI in adult media.\\n\\n\\n== Ethical concerns and misuse ==\\nThe growth of generative AI pornography has also attracted some cause for criticism. AI technology can be exploited to create non-consensual pornographic material, posing risks similar to those seen with deepfake revenge porn and AI-generated NCII (Non-Consen\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "docs=WikipediaLoader(query=\"Generative AI\",load_max_docs=2).load()\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375bed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (.venv)",
   "language": "python",
   "name": "my-venv-311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
