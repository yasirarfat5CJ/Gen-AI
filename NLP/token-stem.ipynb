{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e1fd6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/yasirarfat18/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[32m      8\u001b[39m corpus = \u001b[33m\"\"\"\u001b[39m\u001b[33mHello welcome., \u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33mmyself yasir arfat on the way of learning \u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33mNLP, ML, DL.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m sent=\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(sent)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sent:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/yasirarfat18/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"punkt\")   # download sentence tokenizer\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus = \"\"\"Hello welcome., \n",
    "myself yasir arfat on the way of learning \n",
    "NLP, ML, DL.\"\"\"\n",
    "sent=sent_tokenize(corpus)\n",
    "print(sent)\n",
    "for s in sent:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5b2270b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome.', ',', 'myself', 'yasir', 'arfat', 'on', 'the', 'way', 'of', 'learning', 'NLP', ',', 'ML', ',', 'DL', '.']\n"
     ]
    }
   ],
   "source": [
    "##sent to word  or para to word\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)\n",
    "for s in sent:\n",
    "    print(word_tokenize(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a714ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome.', ',', 'myself', 'Yasir', 'Arfat', 'on', 'the', 'way', 'of', 'learning', 'NLP', ',', 'ML', ',', 'DL', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()   # create tokenizer object\n",
    "tokens = tokenizer.tokenize(\"Hello welcome., myself Yasir Arfat on the way of learning NLP, ML, DL.\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1987f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellos---->hello\n",
      "welcomess.---->welcomess.\n",
      ",---->,\n",
      "myselfs---->myself\n",
      "yasirs---->yasir\n",
      "arfats---->arfat\n",
      "on---->on\n",
      "the---->the\n",
      "ways---->way\n",
      "of---->of\n",
      "learnings---->learn\n",
      "NLPs---->nlp\n",
      "MLs---->ml\n",
      "DLs---->dl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Stemming \n",
    "words=['Hellos', 'welcomess.', ',', 'myselfs', 'yasirs', 'arfats', 'on', 'the', 'ways', 'of', 'learnings', 'NLPs', 'MLs', 'DLs']\n",
    "\n",
    "#PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemming=PorterStemmer()\n",
    "for word in words:\n",
    "    print(word + \"---->\" + stemming.stem(word))\n",
    "stemming.stem(\"history\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51dd5ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RegexpStemmer: 'ing|s$|e$'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer=RegexpStemmer('ing|s$|e$',min=4)\n",
    "print(reg_stemmer)\n",
    "reg_stemmer.stem(\"ingeating\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "696643a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -----> run\n",
      "studies -----> studi\n",
      "easily -----> easili\n",
      "flies -----> fli\n",
      "working -----> work\n",
      "sportingly -----> sport\n",
      "goes -----> goe\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "words = [\"running\", \"studies\", \"easily\", \"flies\", \"working\",\"sportingly\",\"goes\"]  # example word list\n",
    "\n",
    "s = SnowballStemmer('english')\n",
    "\n",
    "for word in words:\n",
    "    print(word + \" -----> \" + s.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b269a85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'P', 'J']\n",
      "['A', 'b', 'd', 'u', 'l']\n",
      "['K', 'a', 'l', 'a', 'm']\n",
      "['w', 'a', 's']\n",
      "['a']\n",
      "['g', 'r', 'e', 'a', 't']\n",
      "['s', 'c', 'i', 'e', 'n', 't', 'i', 's', 't']\n",
      "['a', 'n', 'd']\n",
      "['t', 'h', 'e']\n",
      "['1', '1', 't', 'h']\n",
      "['P', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't']\n",
      "['o', 'f']\n",
      "['I', 'n', 'd', 'i', 'a']\n",
      "['.']\n",
      "['H', 'e']\n",
      "['w', 'a', 's']\n",
      "['k', 'n', 'o', 'w', 'n']\n",
      "['a', 's']\n",
      "['t', 'h', 'e']\n",
      "['M', 'i', 's', 's', 'i', 'l', 'e']\n",
      "['M', 'a', 'n']\n",
      "['o', 'f']\n",
      "['I', 'n', 'd', 'i', 'a']\n",
      "['f', 'o', 'r']\n",
      "['h', 'i', 's']\n",
      "['c', 'o', 'n', 't', 'r', 'i', 'b', 'u', 't', 'i', 'o', 'n']\n",
      "['t', 'o']\n",
      "['s', 'p', 'a', 'c', 'e']\n",
      "['a', 'n', 'd']\n",
      "['d', 'e', 'f', 'e', 'n', 's', 'e']\n",
      "['t', 'e', 'c', 'h', 'n', 'o', 'l', 'o', 'g', 'y']\n",
      "['.']\n",
      "['H', 'e']\n",
      "['i', 'n', 's', 'p', 'i', 'r', 'e', 'd']\n",
      "['m', 'i', 'l', 'l', 'i', 'o', 'n', 's']\n",
      "['o', 'f']\n",
      "['s', 't', 'u', 'd', 'e', 'n', 't', 's']\n",
      "['w', 'i', 't', 'h']\n",
      "['h', 'i', 's']\n",
      "['s', 'i', 'm', 'p', 'l', 'e']\n",
      "['l', 'i', 'f', 'e']\n",
      "[',']\n",
      "['d', 'e', 'd', 'i', 'c', 'a', 't', 'i', 'o', 'n']\n",
      "[',']\n",
      "['a', 'n', 'd']\n",
      "['v', 'i', 's', 'i', 'o', 'n']\n",
      "['f', 'o', 'r']\n",
      "['a']\n",
      "['d', 'e', 'v', 'e', 'l', 'o', 'p', 'e', 'd']\n",
      "['I', 'n', 'd', 'i', 'a']\n",
      "['.']\n",
      "['APJ Abdul Kalam was a great scientist and the 11th President of India.', 'He was known as the Missile Man of India for his contribution to space and defense technology.', 'He inspired millions of students with his simple life, dedication, and vision for a developed India.']\n"
     ]
    }
   ],
   "source": [
    "corpus=\"APJ Abdul Kalam was a great scientist and the 11th President of India. He was known as the Missile Man of India for his contribution to space and defense technology. He inspired millions of students with his simple life, dedication, and vision for a developed India.\"\n",
    "\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "sentences=nltk.sent_tokenize(corpus)\n",
    "words=word_tokenize(corpus)\n",
    "for word in words:\n",
    "    print(word)\n",
    "\n",
    "print(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e37f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping -----> skip\n",
      "Runnnig -----> runnnig\n",
      "dancing -----> danc\n",
      "batting -----> bat\n",
      "allrounder -----> allround\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = ['skipping', 'Runnnig', 'dancing', 'batting', 'allrounder']\n",
    "\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(w, \"----->\", stemming.stem(w.lower()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467581db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
