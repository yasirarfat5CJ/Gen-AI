{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc3fb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tools creation\n",
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper,ArxivAPIWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ee84435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: Generative pre-trained transformer\n",
      "Summary: A generative pre-trained transformer (GPT) is a type of large language model (LLM) that is widely used in generative AI chatbots. GPTs are based on a deep learning architecture called the transformer. They are pre-trained on large datasets of unlabeled content, and able to generate novel content.\n",
      "OpenAI was the first to apply generative pre-training to the transformer architecture, introducing the GPT-1 model in 2018. The company has since released many bigger GPT models. The popular chatbot ChatGPT, released in late 2022 (using GPT-3.5), was followed by many competitor chatbots using their own generative pre-trained transformers to generate text, such as Gemini, DeepSeek or Claude.\n",
      "GPTs are primarily used to generate text, but can be trained to generate other kinds of data. For example, GPT-4o can process and generate text, images and audio. To improve performance on complex tasks, some GPTs, such as OpenAI o3, allocate more computation time analyzing the problem before generating an output, and are called reasoning models. In 2025, GPT-5 was released with a router that automatically selects whether to use a faster model or slower reasoning model based on the provided task.\n",
      "\n",
      "Page: Transformer (deep learning)\n",
      "Summary: In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. \n",
      "Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.\n",
      "\n",
      "The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. The predecessors of transformers were developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\n",
      "\n",
      "\n",
      "\n",
      "Page: BERT (language model)\n",
      "Summary: Bidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learns to represent text as a sequence of vectors using self-supervised learning. It uses the encoder-only transformer architecture. BERT dramatically improved the state of the art for large language models. As of 2020, BERT is a ubiquitous baseline in natural language processing (NLP) experiments. \n",
      "BERT is trained by masked token prediction and next sentence prediction. With this training, BERT learns contextual, latent representations of tokens in their context, similar to ELMo and GPT-2. It found applications for many natural language processing tasks, such as coreference resolution and polysemy resolution. It improved on ELMo and spawned the study of \"BERTology\", which attempts to interpret what is learned by BERT.\n",
      "BERT was originally implemented in the English language at two model sizes, BERTBASE (110 million parameters) and BERTLARGE (340 million parameters). Both were trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words). The weights were released on GitHub. On March 11, 2020, 24 smaller model\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_wiki=WikipediaAPIWrapper(top_k_results=2,doc_content_chars_max=200)\n",
    "wiki=WikipediaAPIWrapper(api_wrapper=api_wrapper_wiki)\n",
    "print(wiki.run(\"transformers in NLP\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77f469ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper_wiki=ArxivAPIWrapper(top_k_results=2,doc_content_chars_max=200)\n",
    "arxiv=ArxivAPIWrapper(api_wrapper=api_wrapper_wiki)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a21281d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki,arxiv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88ca1a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom tools \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56a959cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "944b8d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f75d8e8d750>, search_kwargs={})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. Create vector store\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# 4. Create retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30d71c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def wiki_search(query: str):\n",
    "    \"\"\"Search Wikipedia\"\"\"\n",
    "    return wiki.run(query)\n",
    "\n",
    "@tool\n",
    "def langsmith_search(query: str):\n",
    "    \"\"\"Search any information about LangSmith\"\"\"\n",
    "    return retriever(query)\n",
    "\n",
    "@tool\n",
    "def arxiv_search(query: str):\n",
    "    \"\"\"Search Arxiv papers\"\"\"\n",
    "    return arxiv.run(query)\n",
    "\n",
    "tools = [wiki_search, arxiv_search, langsmith_search]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c8b19ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1494800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    api_key=groq_api_key\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1afb826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI assistant. \"\n",
    "            \"Use the provided tools and context to answer accurately. \"\n",
    "            \"If you do not know the answer, say you do not know.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64a2b2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f77262c4ad0>, search_kwargs={})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. Create vector store\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# 4. Create retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f772620bc50>, search_kwargs={})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. Create vector store\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# 4. Create retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68fc71fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "    timeout=30\n",
    "    # ... (other params)\n",
    ")\n",
    "agent = create_agent(model, tools=tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke({\"messages\": [{\"type\": \"human\", \"content\": \"what is attention\"}]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects Venv",
   "language": "python",
   "name": "langchain-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
